{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skanda/anaconda3/envs/rl/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D4RL is not installed!\n",
      "Lorl env is not installed!\n"
     ]
    }
   ],
   "source": [
    "from ppo.model import PPO\n",
    "from hca.model import HCAModel\n",
    "from hca.buffer import calculate_mc_returns\n",
    "from dualdice.dd_model import DualDICE\n",
    "from dualdice.return_model import ReturnPredictor\n",
    "from utils import get_env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = \"../checkpoints/hca-dualdice_delayed_GridWorld:test_v4_2023-06-14_01:00:19\"\n",
    "checkpoint_ep = 100000\n",
    "ppo_checkpoint = torch.load(f\"{checkpoint_folder}/ppo_{checkpoint_ep}.pt\")\n",
    "args = ppo_checkpoint[\"args\"]\n",
    "hca_checkpoint = torch.load(f\"{checkpoint_folder}/hca_{checkpoint_ep}.pt\")\n",
    "dd_checkpoint = torch.load(f\"{checkpoint_folder}/dd_{checkpoint_ep}.pt\")\n",
    "ret_checkpoint = torch.load(f\"{checkpoint_folder}/ret_{checkpoint_ep}.pt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══╤═══╤═══╤═══╤═══╕\n",
      "│ . │ * │ . │ G │ * │\n",
      "├───┼───┼───┼───┼───┤\n",
      "│ . │ F │ . │ * │ . │\n",
      "├───┼───┼───┼───┼───┤\n",
      "│ . │ F │ . │ . │ . │\n",
      "├───┼───┼───┼───┼───┤\n",
      "│ . │ F │ * │ F │ . │\n",
      "├───┼───┼───┼───┼───┤\n",
      "│ A │ . │ . │ * │ . │\n",
      "╘═══╧═══╧═══╧═══╧═══╛\n"
     ]
    }
   ],
   "source": [
    "env = get_env(args)\n",
    "\n",
    "if isinstance(env.action_space, gym.spaces.Box):\n",
    "    continuous = True\n",
    "else:\n",
    "    continuous = False\n",
    "\n",
    "if continuous:\n",
    "    action_dim = env.action_space.shape[0]\n",
    "else:\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "if args.env.type == \"gridworld\":\n",
    "    # gridworld env\n",
    "    input_dim = env.observation_space[\"map\"].shape[0] + 1\n",
    "else:\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(args.training.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize PPO policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_cnn = None # just leave this for now\n",
    "agent = PPO(input_dim, action_dim, args.agent.lr, continuous, device, args, ppo_cnn)\n",
    "agent.load(ppo_checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize HCA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hca_cnn = None # just leave this for now\n",
    "h_model = HCAModel(\n",
    "            input_dim + 1,  # +1 is for return-conditioned\n",
    "            action_dim,\n",
    "            continuous=continuous,\n",
    "            cnn_base=hca_cnn,\n",
    "            n_layers=args.agent.hca_n_layers,\n",
    "            hidden_size=args.agent.hca_hidden_size,\n",
    "            activation_fn=args.agent.hca_activation,\n",
    "            dropout_p=args.agent.hca_dropout,\n",
    "            batch_size=args.agent.hca_batchsize,\n",
    "            lr=args.agent.hca_lr,\n",
    "            device=args.training.device,\n",
    "            normalize_inputs=args.agent.hca_normalize_inputs,\n",
    "            normalize_return_inputs_only=args.agent.hca_normalize_return_inputs_only,\n",
    "            max_grad_norm=args.agent.hca_max_grad_norm,\n",
    "            weight_training_samples=args.agent.hca_weight_training_samples,\n",
    "            noise_std=args.agent.hca_noise_std,\n",
    "        )\n",
    "h_model.load(hca_checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load DualDICE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_act_dim = action_dim if continuous else 1\n",
    "dd_cnn = None # just leave this for now\n",
    "dd_model = DualDICE(\n",
    "            input_dim,\n",
    "            action_dim=dd_act_dim,\n",
    "            cnn_base=dd_cnn,  # using different CNNs here not worried about compute\n",
    "            f=args.agent.dd_f,\n",
    "            c=1, # args.agent.dd_c when available\n",
    "            n_layers=args.agent.hca_n_layers,\n",
    "            hidden_size=args.agent.hca_hidden_size,\n",
    "            activation_fn=args.agent.hca_activation,\n",
    "            dropout_p=args.agent.hca_dropout,\n",
    "            batch_size=args.agent.hca_batchsize,\n",
    "            lr=args.agent.hca_lr,\n",
    "            device=args.training.device,\n",
    "            normalize_inputs=args.agent.hca_normalize_inputs,\n",
    "            normalize_return_inputs_only=args.agent.hca_normalize_return_inputs_only,\n",
    "            max_grad_norm=args.agent.dd_max_grad_norm,\n",
    "        )\n",
    "dd_model.load(dd_checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_cnn = None # just leave this for now\n",
    "r_model = ReturnPredictor(\n",
    "            input_dim,\n",
    "            quantize=args.agent.r_quant,\n",
    "            num_classes=args.agent.r_num_classes,\n",
    "            cnn_base=r_cnn,  # using different CNNs here not worried about compute\n",
    "            n_layers=args.agent.hca_n_layers,\n",
    "            hidden_size=args.agent.hca_hidden_size,\n",
    "            activation_fn=args.agent.hca_activation,\n",
    "            dropout_p=args.agent.hca_dropout,\n",
    "            batch_size=args.agent.hca_batchsize,\n",
    "            lr=args.agent.hca_lr,\n",
    "            device=args.training.device,\n",
    "            normalize_inputs=args.agent.hca_normalize_inputs,\n",
    "            normalize_targets=args.agent.r_normalize_targets,\n",
    "            max_grad_norm=args.agent.r_max_grad_norm,\n",
    "        )\n",
    "r_model.load(ret_checkpoint)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get hindsight ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hindsight_ratios(state, action, returns):\n",
    "    state = torch.from_numpy(state).reshape(1, -1).float().to(device)\n",
    "    action = torch.from_numpy(action).reshape(1, -1).float().to(device)\n",
    "    returns = torch.Tensor([returns]).reshape(1, -1).float().to(device)\n",
    "    \n",
    "    # Policy value\n",
    "    pi_logprobs, _, _ = agent.policy.evaluate(state, action)\n",
    "    pi_logprobs = pi_logprobs.detach().cpu().numpy()\n",
    "    pi_prob = np.exp(pi_logprobs)\n",
    "\n",
    "    # Hindsight model value\n",
    "    h_logprobs = h_model.get_hindsight_logprobs(state, returns, action).cpu().numpy()\n",
    "    h_prob = np.exp(h_logprobs)\n",
    "\n",
    "    # H-DICE model value\n",
    "    hdice_density_ratio = dd_model.get_density_ratios(state, action, returns)\n",
    "    ret_prob = r_model.get_return_probs(state, returns)\n",
    "\n",
    "    return {\"pi\": pi_prob, \"h\": h_prob, \"naive_ratio\": np.exp(pi_logprobs - h_logprobs), \"dd\": hdice_density_ratio, \"ret\": ret_prob, \"hdice_ratio\": hdice_density_ratio * ret_prob}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect an episode trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_episode(env, ep_actions = []):\n",
    "    state = env.reset()\n",
    "        \n",
    "    done = False\n",
    "\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    terminals = []\n",
    "    ep_len = 0\n",
    "\n",
    "    gn_ep_actions = (ep_actions != [])\n",
    "\n",
    "    while True:\n",
    "        if gn_ep_actions:\n",
    "            action = ep_actions.pop()\n",
    "        else:\n",
    "            # select action with policy\n",
    "            action, _ = agent.select_action(state, greedy=True)\n",
    "        if continuous:\n",
    "            action = action.numpy().flatten()\n",
    "            action = action.clip(\n",
    "                env.action_space.low, env.action_space.high\n",
    "            )\n",
    "        else:\n",
    "            action = action.item() if not isinstance(action, int) else action\n",
    "\n",
    "        # Step in env\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        \n",
    "        # saving reward and terminals\n",
    "        rewards.append(float(reward))\n",
    "        terminals.append(done)\n",
    "\n",
    "        ep_len += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        if (gn_ep_actions and not ep_actions):\n",
    "            break\n",
    "\n",
    "    assert ep_len == len(actions)\n",
    "    \n",
    "    return states, actions, rewards, terminals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridWorld Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 1\n",
    "RIGHT = 0\n",
    "UP = 3\n",
    "DOWN = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══╤═══╤═══╤═══╤═══╕\n",
      "│ . │ * │ . │ G │ * │\n",
      "├───┼───┼───┼───┼───┤\n",
      "│ . │ F │ A │ * │ . │\n",
      "├───┼───┼───┼───┼───┤\n",
      "│ . │ F │ . │ . │ . │\n",
      "├───┼───┼───┼───┼───┤\n",
      "│ . │ F │ . │ F │ . │\n",
      "├───┼───┼───┼───┼───┤\n",
      "│ S │ . │ . │ * │ . │\n",
      "╘═══╧═══╧═══╧═══╧═══╛\n"
     ]
    }
   ],
   "source": [
    "ep1 = [RIGHT, RIGHT, UP, UP, UP]\n",
    "env.reset()\n",
    "for action in ep1:\n",
    "    env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pi': array([[0.00220553]], dtype=float32), 'h': array([0.34851924], dtype=float32), 'naive_ratio': array([[0.00632828]], dtype=float32), 'dd': tensor([[1.0000]]), 'ret': tensor([[0.1390]], dtype=torch.float64), 'hdice_ratio': tensor([[0.1390]], dtype=torch.float64)}\n",
      "{'pi': array([[0.00220553]], dtype=float32), 'h': array([0.1630644], dtype=float32), 'naive_ratio': array([[0.0135255]], dtype=float32), 'dd': tensor([[0.0364]]), 'ret': tensor([[0.3982]], dtype=torch.float64), 'hdice_ratio': tensor([[0.0145]], dtype=torch.float64)}\n",
      "{'pi': array([[0.84563416]], dtype=float32), 'h': array([0.21039516], dtype=float32), 'naive_ratio': array([[4.019266]], dtype=float32), 'dd': tensor([[1.0000]]), 'ret': tensor([[0.1390]], dtype=torch.float64), 'hdice_ratio': tensor([[0.1390]], dtype=torch.float64)}\n",
      "{'pi': array([[0.84563416]], dtype=float32), 'h': array([0.4338784], dtype=float32), 'naive_ratio': array([[1.9490119]], dtype=float32), 'dd': tensor([[0.0355]]), 'ret': tensor([[0.3982]], dtype=torch.float64), 'hdice_ratio': tensor([[0.0141]], dtype=torch.float64)}\n"
     ]
    }
   ],
   "source": [
    "curr_state = env.get_state()\n",
    "print(get_hindsight_ratios(curr_state, np.array([LEFT]), -100))\n",
    "print(get_hindsight_ratios(curr_state, np.array([LEFT]), 69))\n",
    "print(get_hindsight_ratios(curr_state, np.array([RIGHT]), -100))\n",
    "print(get_hindsight_ratios(curr_state, np.array([RIGHT]), 69))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══╤═══╤═══╤═══╤═══╕\n",
      "│ . │ * │ . │ G │ * │\n",
      "├───┼───┼───┼───┼───┤\n",
      "│ . │ F │ . │ * │ . │\n",
      "├───┼───┼───┼───┼───┤\n",
      "│ . │ F │ . │ A │ . │\n",
      "├───┼───┼───┼───┼───┤\n",
      "│ . │ F │ . │ F │ . │\n",
      "├───┼───┼───┼───┼───┤\n",
      "│ S │ . │ . │ * │ . │\n",
      "╘═══╧═══╧═══╧═══╧═══╛\n"
     ]
    }
   ],
   "source": [
    "ep2 = [RIGHT, RIGHT, UP, UP, RIGHT]\n",
    "env.reset()\n",
    "for action in ep2:\n",
    "    env.step(action)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pi': array([[0.6149289]], dtype=float32), 'h': array([0.37544358], dtype=float32), 'naive_ratio': array([[1.6378729]], dtype=float32), 'dd': tensor([[0.0438]]), 'ret': tensor([[0.3971]], dtype=torch.float64), 'hdice_ratio': tensor([[0.0174]], dtype=torch.float64)}\n",
      "{'pi': array([[0.6149289]], dtype=float32), 'h': array([0.13141593], dtype=float32), 'naive_ratio': array([[4.679257]], dtype=float32), 'dd': tensor([[1.0000]]), 'ret': tensor([[0.1465]], dtype=torch.float64), 'hdice_ratio': tensor([[0.1465]], dtype=torch.float64)}\n",
      "{'pi': array([[0.00899117]], dtype=float32), 'h': array([0.09239677], dtype=float32), 'naive_ratio': array([[0.09731042]], dtype=float32), 'dd': tensor([[0.0427]]), 'ret': tensor([[0.3971]], dtype=torch.float64), 'hdice_ratio': tensor([[0.0170]], dtype=torch.float64)}\n",
      "{'pi': array([[0.00899117]], dtype=float32), 'h': array([0.28745288], dtype=float32), 'naive_ratio': array([[0.03127876]], dtype=float32), 'dd': tensor([[1.0000]]), 'ret': tensor([[0.1465]], dtype=torch.float64), 'hdice_ratio': tensor([[0.1465]], dtype=torch.float64)}\n"
     ]
    }
   ],
   "source": [
    "curr_state = env.get_state()\n",
    "print(get_hindsight_ratios(curr_state, np.array([UP]), 69))\n",
    "print(get_hindsight_ratios(curr_state, np.array([UP]), -100))\n",
    "print(get_hindsight_ratios(curr_state, np.array([DOWN]), 69))\n",
    "print(get_hindsight_ratios(curr_state, np.array([DOWN]), -100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pi': array([[0.11634082]], dtype=float32), 'h': array([0.2207028], dtype=float32), 'naive_ratio': array([[0.52713794]], dtype=float32), 'dd': tensor([[0.7403]]), 'ret': tensor([[0.3983]], dtype=torch.float64), 'hdice_ratio': tensor([[0.2949]], dtype=torch.float64)}\n",
      "{'pi': array([[0.04175927]], dtype=float32), 'h': array([0.20499682], dtype=float32), 'naive_ratio': array([[0.20370694]], dtype=float32), 'dd': tensor([[0.6888]]), 'ret': tensor([[0.3681]], dtype=torch.float64), 'hdice_ratio': tensor([[0.2535]], dtype=torch.float64)}\n",
      "{'pi': array([[0.14581066]], dtype=float32), 'h': array([0.2365405], dtype=float32), 'naive_ratio': array([[0.61643004]], dtype=float32), 'dd': tensor([[0.7417]]), 'ret': tensor([[0.3713]], dtype=torch.float64), 'hdice_ratio': tensor([[0.2754]], dtype=torch.float64)}\n",
      "{'pi': array([[0.43238544]], dtype=float32), 'h': array([0.27540377], dtype=float32), 'naive_ratio': array([[1.5700057]], dtype=float32), 'dd': tensor([[0.7354]]), 'ret': tensor([[0.3628]], dtype=torch.float64), 'hdice_ratio': tensor([[0.2668]], dtype=torch.float64)}\n",
      "{'pi': array([[0.3645948]], dtype=float32), 'h': array([0.28240752], dtype=float32), 'naive_ratio': array([[1.2910237]], dtype=float32), 'dd': tensor([[0.7066]]), 'ret': tensor([[0.3391]], dtype=torch.float64), 'hdice_ratio': tensor([[0.2396]], dtype=torch.float64)}\n"
     ]
    }
   ],
   "source": [
    "ep1 = [RIGHT, RIGHT, UP, UP, UP]\n",
    "states, actions, rewards, terminals = get_episode(env, ep1)\n",
    "returns = calculate_mc_returns(rewards, terminals, args.env.gamma)[-1]\n",
    "\n",
    "for s, a in zip(states, actions):\n",
    "    info = get_hindsight_ratios(s, np.array([a]), returns)\n",
    "    print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
