defaults:
  # Set default options
  - _self_
  - agent: ppo
  - env: lorl_state

hydra_base_dir: ""

logger:
  wandb: True
  exp_name_modifier: ""
  group_name_modifier: ""

training:
  device: ""
  max_training_episodes: 100000 # some big number
  seed: 0
  checkpoint: ""
  savedir: ""

env:
  gamma: 0.99

agent:
  update_every: 50
  update_every_env_steps: null
  ppo_epochs: 30
  eps_clip: 0.2
  lamda: 0.95
  entropy_coeff: 0.0
  value_loss_coeff: 0.25
  n_layers: 2
  hidden_size: 64
  lr: 3e-4
  activation_fn: "relu"
  max_grad_norm: null # no clipping by default
  stop_hca: null # set this to some episode number to hard switch from HCA calculation to vanilla PPO. null means never change